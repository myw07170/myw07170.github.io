<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Object Rotation Detection Based on 3D Point Cloud</title>
    <link href="/2024/11/10/Rotation/"/>
    <url>/2024/11/10/Rotation/</url>
    
    <content type="html"><![CDATA[<!-- # Object Rotation Detection Based on 3D Point Cloud --><blockquote><p><em><strong>Undergraduate Thesis</strong><br>Advisor: Prof. Lili Yang<br>Author: Yiwen Mei<br>Oct. 2023-Jun. 2024</em></p></blockquote><h1 id="Brief-Introduction"><a href="#Brief-Introduction" class="headerlink" title="Brief Introduction"></a>Brief Introduction</h1><p>Research objective: Through in-depth analysis and processing of 3D point cloud data, develop a tool that can automatically label the object orientation in the data, that is, predict the orientation Angle of the target object and label it.</p><p  align = "center"><img  src="/img/Rotation/technology_roadmap.png"  width="500"  /></p><center>technology roadmap</center><h1 id="Construction-of-Data-Sets"><a href="#Construction-of-Data-Sets" class="headerlink" title="Construction of Data Sets"></a>Construction of Data Sets</h1><h3 id="Data-set-acquisition-and-annotation"><a href="#Data-set-acquisition-and-annotation" class="headerlink" title="Data set acquisition and annotation"></a>Data set acquisition and annotation</h3><p> <em><strong>1. data collection</strong></em></p><ul><li>The <strong>LiDAR</strong> was used to select different road scenes in different time periods in the campus of China Agricultural University, and the data was collected for three different objects – <strong>pedestrians, cars and cyclists</strong>.</li><li>1043 scene pictures (.jpg) and 3D point cloud data (.txt) of <strong>1986 objects</strong> were collected.<p  align = "center"><img  src="/img/Rotation/scene_picture.png"  width="600"  /></p><center>Scene pictures</center></li></ul><p> <em><strong>2. data format</strong></em></p><ul><li>The 3D point cloud data of the collected 1986 objects are saved in ‘’.txt’’ format.</li><li>Each file contains three fields representing the x,y, and z coordinates of each point in its point cloud.<!-- <p  align = "center"><img  src="/img/Rotation/data_format.png"  width="400"  /></p><center>data format</center> --></li></ul><p> <em><strong>3. data annotation</strong></em></p><ul><li>According to the shot picture and visualization point cloud of each target, arbitrarily adjust the Angle of view to judge its orientation.</li><li>After adjusting to the <strong>overlooking Angle</strong>, mark the Angle. After marking, add <strong>the fourth field</strong> – Angle label in the target point cloud data file.</li><li>The 3D point cloud data of <strong>1191 sample objects</strong> marked with orientation Angle is obtained by removing the data that cannot be judged by human beings.</li></ul><h3 id="Data-enhancement"><a href="#Data-enhancement" class="headerlink" title="Data enhancement"></a>Data enhancement</h3><p>Using the <strong>rotation matrix</strong> to achieve data enhancement, a dataset containing <strong>3600</strong> samples is obtained, and there are 10 samples for each Angle orientation.</p><p  align = "center"><img  src="/img/Rotation/data_enhancement.png"  width="600"  /></p><center>data enhancement procedure</center><h1 id="3D-point-cloud-object-rotation-detection-based-on-Pointnet"><a href="#3D-point-cloud-object-rotation-detection-based-on-Pointnet" class="headerlink" title="3D point cloud object rotation detection based on Pointnet ++"></a>3D point cloud object rotation detection based on Pointnet ++</h1><h3 id="Data-preprocessing-The-point-cloud-number-of-each-sample-in-the-data-set-is-unified-to-300"><a href="#Data-preprocessing-The-point-cloud-number-of-each-sample-in-the-data-set-is-unified-to-300" class="headerlink" title="Data preprocessing: The point cloud number of each sample in the data set is unified to 300."></a>Data preprocessing: The point cloud number of each sample in the data set is unified to 300.</h3><ul><li>Reduce points: FPS algorithm combined with K-Means clustering</li><li>Add points: Linear interpolation algorithm<p  align = "center"><img  src="/img/Rotation/reduce_points.png"  width="450"  /></p><center>e.g. reduce points</center></li></ul><p  align = "center"><img  src="/img/Rotation/add_points.png"  height="250"  /></p><center>e.g. add points</center><h3 id="Model-training-results"><a href="#Model-training-results" class="headerlink" title="Model training results"></a>Model training results</h3><h4 id="Training-process-and-results-of-data-set-1-12-classification"><a href="#Training-process-and-results-of-data-set-1-12-classification" class="headerlink" title="Training process and results of data set 1 (12 classification)"></a>Training process and results of data set 1 (12 classification)</h4><!-- <p  align = "center"><img  src="/img/Rotation/12-acc.png"  height="250"  /></p><center></center><p  align = "center"><img  src="/img/Rotation/12-err.png"  height="250"  /></p><center></center> --><style>.center {  width: auto;  display: table;  margin-left: auto;  margin-right: auto;}</style><div class="center"><table><thead><tr><th align="center"></th><th align="center">Accuracy</th><th align="center">Average Angle Error</th></tr></thead><tbody><tr><td align="center">training set</td><td align="center">95.03%</td><td align="center">10.38°</td></tr><tr><td align="center">test set</td><td align="center">84.03%</td><td align="center">14.84°</td></tr></tbody></table></div><h4 id="Training-process-and-results-of-data-set-2-18-classification"><a href="#Training-process-and-results-of-data-set-2-18-classification" class="headerlink" title="Training process and results of data set 2 (18 classification)"></a>Training process and results of data set 2 (18 classification)</h4><!-- <p  align = "center"><img  src="/img/Rotation/18-acc.png"  height="500"  /></p><center></center><p  align = "center"><img  src="/img/Rotation/18-err.png"  height="250"  /></p><center></center> --><div class="center"><table><thead><tr><th align="center"></th><th align="center">Accuracy</th><th align="center">Average Angle Error</th></tr></thead><tbody><tr><td align="center">training set</td><td align="center">90.90%</td><td align="center">8.21°</td></tr><tr><td align="center">test set</td><td align="center">75.69%</td><td align="center">12.53°</td></tr></tbody></table></div><h3 id="Effect-display"><a href="#Effect-display" class="headerlink" title="Effect display"></a>Effect display</h3><h1 id="Summary-and-prospect"><a href="#Summary-and-prospect" class="headerlink" title="Summary and prospect"></a>Summary and prospect</h1>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Classification of Lettuce Nitrogen Levels</title>
    <link href="/2024/11/09/Features/"/>
    <url>/2024/11/09/Features/</url>
    
    <content type="html"><![CDATA[<!-- # Classification of Lettuce Nitrogen Levels Based on the Integration of Hyperspectral and Image Features --><blockquote><p><em><strong>National College Student Innovation and Entrepreneurship Project</strong></em><br><em>Advisor: Prof. Minjuan Wang</em><br><em>Team member: Xiaohan Wan, <strong>Yiwen Mei</strong>, Zijun Gao</em><br><em>May 2023-Apr. 2024</em></p></blockquote><h1 id="Project-contents"><a href="#Project-contents" class="headerlink" title="Project contents"></a>Project contents</h1><ul><li><p>The suitable nitrogen level has an important effect on the production quality of leaf vegetables. The accuracy of single image analysis of leaf dishes is not high, so it is necessary to study the nitrogen stress level of leaf dishes by various methods. In this project, the plant images under different nitrogen stress were collected, and the classification method of nitrogen stress degree of leaf vegetables was improved through the combination of hyperspectral and image characteristics.</p></li><li><p>The purpose of this project is to explore a rapid and accurate estimation method of nitrogen stress degree of rosella leaves, enrich the theoretical basis of rosella cultivation in plant factories, and have a good practical application prospect for intelligent regulation of nutrient solution composition, monitoring growth, predicting yield and quality in plant factories.</p><!-- ![植物工厂图片](/img/National/Plant-factory.png "Plant factory") --><p align = "center">    <img  src="/img/National/Plant-factory.png" width="400" /></p><center>Plant factory</center></li></ul><h1 id="Completions"><a href="#Completions" class="headerlink" title="Completions"></a>Completions</h1><h3 id="Plant-experiment-and-data-acquisition"><a href="#Plant-experiment-and-data-acquisition" class="headerlink" title="Plant experiment and data acquisition"></a>Plant experiment and data acquisition</h3><ul><li>Two crop planting experiments were carried out in the plant factory of the Institute of Information and Telecommunications to obtain the plant data set under different nitrogen concentrations</li></ul><!-- ![作物培养过程](/img/National/Crop_culture_process.png  "Crop culture process") --><p align = "center">    <img  src="/img/National/Crop_culture_process.png" width="600" /></p><center>Crop culture process</center>  <ul><li>High spectral spectrometer is used to obtain RGB images and hyperspectral data</li></ul><!-- ![高光谱影像系统](/img/National/Hyperspectral_imaging_system.png  "Hyperspectral imaging system") --><p align = "center">    <img  src="/img/National/Hyperspectral_imaging_system.png" height="300" /></p><center>Hyperspectral imaging system</center>  <ul><li>Three chemical experiments were carried out in the plant protection building laboratory, and the actual nitrogen content of plants was obtained by Kjeldahl nitrogen determination method</li></ul><!-- ![实验测定氮含量](/img/National/nitrogen_content.png  "The nitrogen content was determined by experiment") --><p align = "center">    <img  src="/img/National/nitrogen_content.png" height="350" /></p><center>The nitrogen content was determined by experiment</center>  <h2 id="Research-content-1-RGB-image-processing-classification-model-comparison"><a href="#Research-content-1-RGB-image-processing-classification-model-comparison" class="headerlink" title="Research content 1: RGB image processing, classification model comparison"></a>Research content 1: RGB image processing, classification model comparison</h2><ol><li><p>The U-Net network model is used to segment the blade data set</p><!-- ![U-Net网络结构](/img/National/U-Net.png  "U-Net structure") --><!-- ![处理前后对比](/img/National/Comparison.png  "Comparison of image samples before and after processing") --><p align = "center">    <img  src="/img/National/Comparison.png" height="350" /></p><center>Comparison of image samples before and after processing</center></li><li><p>Different classification models</p><style>.center {  width: auto;  display: table;  margin-left: auto;  margin-right: auto;}</style></li></ol><div class="center"><table><thead><tr><th align="center">Model</th><th align="center">Average accuracy</th></tr></thead><tbody><tr><td align="center">VGG16</td><td align="center">52%</td></tr><tr><td align="center">GoogleNet</td><td align="center">42%</td></tr><tr><td align="center">AlexNet</td><td align="center">34%</td></tr><tr><td align="center">ResNet34</td><td align="center">32%</td></tr></tbody></table></div><h2 id="Research-content-2-hyperspectral-data-processing-classification-model-comparison"><a href="#Research-content-2-hyperspectral-data-processing-classification-model-comparison" class="headerlink" title="Research content 2: hyperspectral data processing, classification model comparison"></a>Research content 2: hyperspectral data processing, classification model comparison</h2><ol><li>PCA(principal component analysis)<!-- ![高光谱数据处理](/img/National/Hyperspectral.png  "Hyperspectral data processing") --><p align = "center">    <img  src="/img/National/Hyperspectral.png" width="650" /></p><center>Hyperspectral data processing</center></li></ol><!-- ![不同波段的贡献度](/img/National/different_bands.png  "The contribution of different bands") --><p align = "center">    <img  src="/img/National/different_bands.png" height="350" /></p><center>The contribution of different bands</center><ol start="2"><li>support vector machine regression<!-- ![支持向量机回归](/img/National/support_vector.png  "support vector") --><p align = "center">    <img  src="/img/National/support_vector.png" height="350" /></p><center>support vector</center></li></ol><div class="center"><table><thead><tr><th align="center">Data Set</th><th align="center">Accuracy</th><th align="center">Recall</th></tr></thead><tbody><tr><td align="center">original</td><td align="center">0.31</td><td align="center">0.33</td></tr><tr><td align="center">PC1, PC2</td><td align="center">0.54</td><td align="center">0.53</td></tr></tbody></table></div><h2 id="Research-content-3-Establish-a-feature-fusion-model-based-on-hyperspectral-and-RGB-images"><a href="#Research-content-3-Establish-a-feature-fusion-model-based-on-hyperspectral-and-RGB-images" class="headerlink" title="Research content 3: Establish a feature fusion model based on hyperspectral and RGB images"></a>Research content 3: Establish a feature fusion model based on hyperspectral and RGB images</h2><ol><li><p>Image texture feature extraction: ASM (angular second order moment), Con (contrast), Ene (energy), Hom (homogeneity), Dis (Dissimilarity), Cor (correlation)</p></li><li><p>Hyperspectral feature band selection: Take out the absolute value of the average weight coefficient of each wavelength to obtain the actual contribution degree of each feature.</p><!-- ![对PC1进行特征波段分析](/img/National/PC1_band.png  "PC1 characteristic band analysis")  --><p align = "center">    <img  src="/img/National/PC1_band.png" width="650" /></p><center>PC1 characteristic band analysis</center></li><li><p>Fusion model establishment: A feature fusion model based on convolutional neural network is designed to effectively fuse the features of RGB image and HSI data to improve image classification performance</p><!-- ![基于CNN的特征融合模型](/img/National/feature_fusion_model.png  "Feature fusion model based on CNN") --><p align = "center">    <img  src="/img/National/feature_fusion_model.png" width="900" /></p><center>Feature fusion model based on CNN</center></li></ol><!-- ![基于CNN的特征融合模型-acc](/img/National/Accuracy.png  "Accuracy")![基于CNN的特征融合模型-loss](/img/National/Loss.png  "Loss") --><p align = "center">    <img  src="/img/National/Accuracy.png" height="350" /></p><center>Accuracy</center><p align = "center">    <img  src="/img/National/Loss.png" height="350" /></p><center>Loss</center><h1 id="Innovation-Points"><a href="#Innovation-Points" class="headerlink" title="Innovation Points"></a>Innovation Points</h1><h4 id="1-The-qualitative-estimation-of-nitrogen-content-of-roselle-based-on-RGB-image-was-realized"><a href="#1-The-qualitative-estimation-of-nitrogen-content-of-roselle-based-on-RGB-image-was-realized" class="headerlink" title="1. The qualitative estimation of nitrogen content of roselle based on RGB image was realized"></a>1. The qualitative estimation of nitrogen content of roselle based on RGB image was realized</h4><ul><li>Establish multiple deep learning methods</li><li>To provide qualitative reference for nitrogen content in romaine rapeseed leaves</li></ul><h4 id="2-The-quantitative-estimation-of-nitrogen-content-in-romaine-was-realized-based-on-hyperspectrum"><a href="#2-The-quantitative-estimation-of-nitrogen-content-in-romaine-was-realized-based-on-hyperspectrum" class="headerlink" title="2. The quantitative estimation of nitrogen content in romaine was realized based on hyperspectrum"></a>2. The quantitative estimation of nitrogen content in romaine was realized based on hyperspectrum</h4><ul><li>The spectral preprocessing and characteristic wavelength extraction were realized by statistical principle</li><li>The actual nitrogen content of plants was obtained by stoichiometry</li><li>A quantitative estimation model of nitrogen content in rosella was established with characteristic wavelength</li></ul><h4 id="3-A-fusion-estimation-model-of-nitrogen-stress-in-rosellas-was-established"><a href="#3-A-fusion-estimation-model-of-nitrogen-stress-in-rosellas-was-established" class="headerlink" title="3. A fusion estimation model of nitrogen stress in rosellas was established"></a>3. A fusion estimation model of nitrogen stress in rosellas was established</h4><ul><li>A feature fusion model based on convolutional neural network is designed to fuse the features of RGB image and hyperspectral data effectively to improve the performance of image classification</li><li>Using CNN as the infrastructure and designing the feature extraction part and classifier part, the accuracy of nitrogen content estimation model is higher than that of single image or hyperspectral model</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
