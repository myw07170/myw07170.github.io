<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Object Rotation Detection Based on 3D Point Cloud</title>
    <link href="/2024/11/10/Rotation/"/>
    <url>/2024/11/10/Rotation/</url>
    
    <content type="html"><![CDATA[<!-- # Object Rotation Detection Based on 3D Point Cloud --><blockquote><p><em><strong>Undergraduate Thesis</strong><br>Advisor: Prof. Lili Yang<br>Author: Yiwen Mei<br>Oct. 2023-Jun. 2024</em></p></blockquote><h1 id="1-Brief-Introduction"><a href="#1-Brief-Introduction" class="headerlink" title="1. Brief Introduction"></a>1. Brief Introduction</h1><ul><li><p>Research objective: Through in-depth analysis and processing of 3D point cloud data, develop a tool that can automatically label the object orientation in the data, that is, predict the orientation Angle of the target object and label it.</p></li><li><p>The technical route of this research is shown in the figure below, and the research background, research purpose and research significance are clearly defined in the project selection stage. Then I looked up and learned relevant literature. Through careful study of literature, I learned the theoretical basis and model framework of relevant algorithms, and tried to reproduce the model. At the same time, self-built data sets were created and processed. Relevant data were first collected, manually marked, then data enhancement and scale unification were carried out, and then data set formats were adjusted, categories were divided, and training sets and test sets were divided according to experimental needs. Finally, the selected model framework is trained with the created data set, and the relevant data of the final training results are obtained after repeated inspection and adjustment.</p></li></ul><p  align = "center"><img  src="/img/Rotation/technology_roadmap.png"  width="450"  /></p><center>technology roadmap</center><h1 id="2-Construction-of-Data-Sets"><a href="#2-Construction-of-Data-Sets" class="headerlink" title="2. Construction of Data Sets"></a>2. Construction of Data Sets</h1><h3 id="2-1-Data-set-acquisition-and-annotation"><a href="#2-1-Data-set-acquisition-and-annotation" class="headerlink" title="2.1 Data set acquisition and annotation"></a>2.1 Data set acquisition and annotation</h3><p> <em><strong>1. data collection</strong></em></p><ul><li><strong>Lidar</strong> technology was used to select diverse road scenes in different time periods on the campus of China Agricultural University, and data were collected for three different common road objects – <strong>pedestrians, cars and cyclists</strong>. The collected data were systematically classified and saved in the folder “imageFile” and “pointCloudFile” respectively. The imageFile folder stores diversified scene images in.jpg format, including <strong>daytime, evening, and night</strong> road scene images. The “pointCloudFile” folder stores the 3D point cloud data of each target object in the form of a.txt text file. Each text file contains three fields, which are the x,y and z coordinates of each point in its point cloud, and these coordinate information accurately describe the position and shape of the target object in space.</li></ul><p  align = "center"><img  src="/img/Rotation/scene_picture.png"  width="600"  /></p><center>Scene pictures</center><ul><li>Through the above data collection and storage process, a high-quality original data set is constructed in this paper. The dataset contains a total of 1,043 images covering a diverse range of road environments and lighting conditions, as well as 3D point cloud data for a total of <strong>1,986 objects</strong>, which fall into three categories: pedestrians, cars and cyclists. The establishment of this data set provides a solid foundation for the subsequent data analysis and model training.</li></ul><style>.center {  width: auto;  display: table;  margin-left: auto;  margin-right: auto;}</style><div class="center"><table><thead><tr><th align="center"></th><th align="center">daytime</th><th align="center">evening</th><th align="center">night</th><th align="center">totality</th></tr></thead><tbody><tr><td align="center">pedestrians</td><td align="center">290</td><td align="center">299</td><td align="center">621</td><td align="center">1210</td></tr><tr><td align="center">cars</td><td align="center">125</td><td align="center">56</td><td align="center">169</td><td align="center">350</td></tr><tr><td align="center">cyclists</td><td align="center">98</td><td align="center">121</td><td align="center">207</td><td align="center">426</td></tr><tr><td align="center">totality</td><td align="center">513</td><td align="center">476</td><td align="center">997</td><td align="center">1986</td></tr></tbody></table></div> <!-- ***2. data format**** The 3D point cloud data of the collected 1986 objects are saved in ''.txt'' format.* Each file contains three fields representing the x,y, and z coordinates of each point in its point cloud. --><!-- <p  align = "center"><img  src="/img/Rotation/data_format.png"  width="400"  /></p><center>data format</center> --><p> <em><strong>2. data annotation</strong></em></p><ul><li>According to the shot picture and visualization point cloud of each target, arbitrarily adjust the Angle of view to judge its orientation.</li><li>After adjusting to the <strong>overlooking Angle</strong>, mark the Angle. After marking, add <strong>the fourth field</strong> – Angle label in the target point cloud data file.</li><li>The 3D point cloud data of <strong>1191 sample objects</strong> marked with orientation Angle is obtained by removing the data that cannot be judged by human beings.</li></ul><div class="center"><table><thead><tr><th align="center"></th><th align="center">daytime</th><th align="center">evening</th><th align="center">night</th><th align="center">totality</th></tr></thead><tbody><tr><td align="center">pedestrians</td><td align="center">176</td><td align="center">54</td><td align="center">292</td><td align="center">522</td></tr><tr><td align="center">cars</td><td align="center">123</td><td align="center">54</td><td align="center">160</td><td align="center">337</td></tr><tr><td align="center">cyclists</td><td align="center">84</td><td align="center">109</td><td align="center">139</td><td align="center">332</td></tr><tr><td align="center">totality</td><td align="center">383</td><td align="center">217</td><td align="center">591</td><td align="center">1191</td></tr></tbody></table></div><h3 id="2-2-Data-enhancement"><a href="#2-2-Data-enhancement" class="headerlink" title="2.2 Data enhancement"></a>2.2 Data enhancement</h3><p>Using the <strong>rotation matrix</strong> to achieve data enhancement, a dataset containing <strong>3600</strong> samples is obtained, and there are 10 samples for each Angle orientation.</p><p  align = "center"><img  src="/img/Rotation/data_enhancement.png"  width="600"  /></p><center>data enhancement procedure</center><h1 id="3-3D-point-cloud-object-rotation-detection-based-on-Pointnet"><a href="#3-3D-point-cloud-object-rotation-detection-based-on-Pointnet" class="headerlink" title="3. 3D point cloud object rotation detection based on Pointnet ++"></a>3. 3D point cloud object rotation detection based on Pointnet ++</h1><h3 id="3-1-Data-preprocessing-The-point-cloud-number-of-each-sample-in-the-data-set-is-unified-to-300"><a href="#3-1-Data-preprocessing-The-point-cloud-number-of-each-sample-in-the-data-set-is-unified-to-300" class="headerlink" title="3.1 Data preprocessing: The point cloud number of each sample in the data set is unified to 300."></a>3.1 Data preprocessing: The point cloud number of each sample in the data set is unified to 300.</h3><h5 id="Reduce-points-FPS-algorithm-combined-with-K-Means-clustering"><a href="#Reduce-points-FPS-algorithm-combined-with-K-Means-clustering" class="headerlink" title="Reduce points: FPS algorithm combined with K-Means clustering"></a>Reduce points: FPS algorithm combined with K-Means clustering</h5><p>Farthest Point Sampling (FPS) algorithm is a common and effective method to reduce the number of point cloud data points. The algorithm constructs a representative point set by iteratively selecting the point farthest from the current point set, thereby simplifying and homogenizing the distribution of data points. In order to further optimize the effect of FPS algorithm, this study introduces K-Means clustering method to improve the selection process of initial points. Specifically, firstly, the K-means algorithm is used to cluster the point cloud data and generate K initial centroid points. FPS sampling is then performed based on these centroid points, thus ensuring that the number of points is effectively reduced while the object’s contour features are preserved. This FPS algorithm combined with K-Means clustering can significantly improve the rationality and efficiency of data point selection.</p><p  align = "center"><img  src="/img/Rotation/reduce_points.png"  width="450"  /></p><center>e.g. reduce points</center><h4 id="Add-points-Linear-interpolation-algorithm"><a href="#Add-points-Linear-interpolation-algorithm" class="headerlink" title="Add points: Linear interpolation algorithm"></a>Add points: Linear interpolation algorithm</h4><p>The purpose of adding points is to improve the accuracy of the model and the ability to capture detail. This process can be accomplished by simply copying existing data points, but a more precise and efficient way can be achieved by using linear interpolation techniques. Linear interpolation is a mature mathematical method that estimates the value of unknown points by constructing straight lines or curves between known data points, thus effectively filling in the blank areas of the data, thereby enhancing the detail representation and overall accuracy of the model.</p><p  align = "center"><img  src="/img/Rotation/add_points.png"  height="250"  /></p><center>e.g. add points</center><h3 id="3-2-Model-training-results"><a href="#3-2-Model-training-results" class="headerlink" title="3.2 Model training results"></a>3.2 Model training results</h3><h4 id="Training-process-and-results-of-data-set-1-12-classification"><a href="#Training-process-and-results-of-data-set-1-12-classification" class="headerlink" title="Training process and results of data set 1 (12 classification)"></a>Training process and results of data set 1 (12 classification)</h4><!-- <p  align = "center"><img  src="/img/Rotation/12-acc.png"  height="250"  /></p><center></center><p  align = "center"><img  src="/img/Rotation/12-err.png"  height="250"  /></p><center></center> --><ul><li>After 300 training iterations, the PointNet++ model achieved significant performance improvements on the self-built dataset 1 above. In order to select the optimal model, this paper evaluates the model based on the accuracy of the test set, and saves the results of the round with the highest accuracy of the test set. The final performance is summarized in the following table, where the training set reaches an accuracy of about 95.03%, indicating that the model effectively learns the features of the data set. The accuracy of the test set is about 84.03%, which reflects the generalization ability of the model on new data.</li><li>In order to evaluate the performance of the model more comprehensively, the average Angle error is further calculated. As can be seen from the table, the average Angle error of the training set is 10.38°, which shows the high precision of the model in classification Angle. The average Angle error of the test set is 14.84°, which is slightly higher than that of the training set, but still within the acceptable range, indicating that the model can still maintain good performance in the face of unseen data. These results provide a valuable reference for the follow-up research.</li></ul><style>.center {  width: auto;  display: table;  margin-left: auto;  margin-right: auto;}</style><div class="center"><table><thead><tr><th align="center"></th><th align="center">Accuracy</th><th align="center">Average Angle Error</th></tr></thead><tbody><tr><td align="center">training set</td><td align="center">95.03%</td><td align="center">10.38°</td></tr><tr><td align="center">test set</td><td align="center">84.03%</td><td align="center">14.84°</td></tr></tbody></table></div><h4 id="Training-process-and-results-of-data-set-2-18-classification"><a href="#Training-process-and-results-of-data-set-2-18-classification" class="headerlink" title="Training process and results of data set 2 (18 classification)"></a>Training process and results of data set 2 (18 classification)</h4><!-- <p  align = "center"><img  src="/img/Rotation/18-acc.png"  height="500"  /></p><center></center><p  align = "center"><img  src="/img/Rotation/18-err.png"  height="250"  /></p><center></center> --><ul><li>After 300 training iterations on the self-built dataset 2, the PointNet++ model also showed significant performance improvements. In order to select the optimal model configuration, this paper evaluates the accuracy of the test set carefully, and saves the results of the round with the highest accuracy of the test set. The final model performance is summarized in the following table: On the training set, the PointNet++ model achieves about 90.90% accuracy, which fully proves that the model can effectively learn and capture key features in dataset 2. On the test set, the model also achieved an accuracy of about 75.69%, which indicates that the model shows strong generalization ability in the face of new data.</li><li>Furthermore, the average Angle error of the training set and the test set is calculated. The average Angle error of the training set is 8.21°, which indicates that the model has a high accuracy in the classification Angle. Although the average Angle error of the test set is 12.53°, which is slightly higher than that of the training set, this value is still within the acceptable range, which once again verifies the good performance of the model on the unseen data. These detailed performance evaluation results not only provide a strong support for the current research, but also provide a valuable reference for the subsequent research work.</li></ul><div class="center"><table><thead><tr><th align="center"></th><th align="center">Accuracy</th><th align="center">Average Angle Error</th></tr></thead><tbody><tr><td align="center">training set</td><td align="center">90.90%</td><td align="center">8.21°</td></tr><tr><td align="center">test set</td><td align="center">75.69%</td><td align="center">12.53°</td></tr></tbody></table></div><p>In summary, compared with dataset 1, the model of dataset 2, although slightly less accurate, showed better performance in terms of average angular error. This result has its inherent logical rationality. Given that the classification accuracy of dataset 2 is set at 20°, compared with the classification accuracy of dataset 1 at 30°, the standard is more refined and the requirements are more stringent, which undoubtedly increases the difficulty of the classification task and leads to a slight decline in the classification accuracy. However, on the other hand, just because data set 2 has higher requirements for classification accuracy, the average Angle error displayed by its model in the prediction process is significantly reduced, which means that the deviation between the predicted value and the real value is effectively controlled. Therefore, for these two training results, we can not simply classify them as good or bad, but should choose and apply the most appropriate model based on the needs of practical applications.</p><h3 id="3-3-Effect-display"><a href="#3-3-Effect-display" class="headerlink" title="3.3 Effect display"></a>3.3 Effect display</h3><h1 id="4-Summary-and-prospect"><a href="#4-Summary-and-prospect" class="headerlink" title="4. Summary and prospect"></a>4. Summary and prospect</h1><h3 id="4-1-Summary"><a href="#4-1-Summary" class="headerlink" title="4.1 Summary"></a>4.1 Summary</h3><p>Through rigorous experimental verification, this study not only validates the effectiveness of Pointnet ++ model in 3D point cloud rotation detection tasks, but also provides technical tools for environment perception and decision making of autonomous driving systems. Specifically, the main academic achievements of this research are as follows:</p><ol><li>A 3D point cloud dataset covering three common objects of pedestrians, cars and cyclists in the road scene was successfully constructed, and the dataset included samples of various rotation angles. Through data enhancement technology, the diversity and distribution balance of data sets are effectively improved, and rich training samples are provided for training models with stronger robustness.</li><li>PointNet++ model was used for training, which was systematically trained and tested on self-built data sets. The experimental results show that the model has good performance in classification accuracy and Angle prediction ability. During the training process, the model showed efficient recognition of each rotation Angle, which further verified the great potential of deep learning in processing 3D point cloud data.</li><li>Active exploration and attempts are made in the rotation Angle prediction of point cloud data. By combining the existing deep learning model with the self-developed data processing method, remarkable results have been achieved in improving the adaptability and accuracy of the model in complex environments, providing an effective tool for the environment perception and decision making of the autonomous driving system.</li></ol><h3 id="4-2-Prospect"><a href="#4-2-Prospect" class="headerlink" title="4.2 Prospect"></a>4.2 Prospect</h3><ol><li>Model optimization: Further explore the direction of model optimization, try to improve the overall performance and reduce the amount of computation</li><li>Accuracy improvement: regression model is introduced to achieve specific numerical prediction of orientation Angle</li><li>Field evaluation: The research results are applied to evaluate the model performance in actual vehicle testing</li><li>……</li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Classification of Lettuce Nitrogen Levels</title>
    <link href="/2024/11/09/Features/"/>
    <url>/2024/11/09/Features/</url>
    
    <content type="html"><![CDATA[<!-- # Classification of Lettuce Nitrogen Levels Based on the Integration of Hyperspectral and Image Features --><blockquote><p><em><strong>National College Student Innovation and Entrepreneurship Project</strong></em><br><em>Advisor: Prof. Minjuan Wang</em><br><em>Team member: Xiaohan Wan, <strong>Yiwen Mei</strong>, Zijun Gao</em><br><em>May 2023-Apr. 2024</em></p></blockquote><h1 id="1-Project-contents"><a href="#1-Project-contents" class="headerlink" title="1. Project contents"></a>1. Project contents</h1><ul><li><p>The suitable nitrogen level has an important effect on the production quality of leaf vegetables. The accuracy of single image analysis of leaf dishes is not high, so it is necessary to study the nitrogen stress level of leaf dishes by various methods. In this project, the plant images under different nitrogen stress were collected, and the classification method of nitrogen stress degree of leaf vegetables was improved through the combination of hyperspectral and image characteristics.</p></li><li><p>The purpose of this project is to explore a rapid and accurate estimation method of nitrogen stress degree of rosella leaves, enrich the theoretical basis of rosella cultivation in plant factories, and have a good practical application prospect for intelligent regulation of nutrient solution composition, monitoring growth, predicting yield and quality in plant factories.</p></li></ul><!-- ![植物工厂图片](/img/National/Plant-factory.png "Plant factory") --><p align = "center">    <img  src="/img/National/Plant-factory.png" width="450" /></p><center>Plant factory</center><h1 id="2-Completions"><a href="#2-Completions" class="headerlink" title="2. Completions"></a>2. Completions</h1><h2 id="2-1-Plant-experiment-and-data-acquisition"><a href="#2-1-Plant-experiment-and-data-acquisition" class="headerlink" title="2.1 Plant experiment and data acquisition"></a>2.1 Plant experiment and data acquisition</h2><ul><li>The four seasons balsamic wheat from Jingnong Research was used as the material in this experiment, which is widely cultivated in the whole country. At present, two batches of rosella planting experiments have been completed in the Plant Factory of the School of Information and Electrical Engineering from June 2023 to August 2023 and from September 2023 to November 2023. The temperature, humidity, CO2 concentration etc. in the plant are adjusted to the best. 1&#x2F;2 Hoagland nutrient solution was used as the substrate. A total of 60 plants were planted in the hydroponic box. In order to collect data of rosella and nitrogen stress, we induced low nitrogen stress and high nitrogen stress by controlling nitrogen concentration in culture medium. The hydroponic box is hung above the plant special light, and the light intensity is controlled by adjusting the knob.</li><li>According to the previous investigation results, nitrogen concentration was divided into three levels: <strong>2.5g&#x2F;L</strong> (weak nitrogen stress), <strong>10.5g&#x2F;L</strong> (suitable nitrogen environment), <strong>18.5g&#x2F;L</strong> (strong nitrogen stress).</li></ul><!-- ![作物培养过程](/img/National/Crop_culture_process.png  "Crop culture process") --><p align = "center">    <img  src="/img/National/Crop_culture_process.png" width="600" /></p><center>Crop culture process</center>  <!-- * During the three growth stages of the plants, the plants were collected and brought back to the laboratory for scanning using the GaiaSorter indoor hyperspectral imaging system of Beijing Zhuolihan Optical Instrument Co., LTD., to obtain the hyperspectral data of the leaves. * The imaging system is equipped with a V10E spectrometer, an OL23 lens, an LT365 detector, two bromo-tungsten light sources and an electronically controlled payload moving platform. The collected spectral range is 382-1026nm, spectral resolution is 2.8nm, sampling interval is 0.65nm, a total of 728 bands. The exposure time of the imaging system is 15ms, and the moving speed of the platform is 0.5mm/s. --><!-- ![高光谱影像系统](/img/National/Hyperspectral_imaging_system.png  "Hyperspectral imaging system") --><!-- <p align = "center">    <img  src="/img/National/Hyperspectral_imaging_system.png" height="300" /></p><center>Hyperspectral imaging system</center> --><p style="width:700px;" >    <img src="/img/National/Hyperspectral_imaging_system.png" style="border: none; box-shadow: none;" align="right" height = "300" hspace="20" vspace="20" >    <p>During the three growth stages of the plants, the plants were collected and brought back to the laboratory for scanning using the GaiaSorter indoor hyperspectral imaging system of Beijing Zhuolihan Optical Instrument Co., LTD., to obtain the hyperspectral data of the leaves. </p>    <p>The imaging system is equipped with a V10E spectrometer, an OL23 lens, an LT365 detector, two bromo-tungsten light sources and an electronically controlled payload moving platform. The collected spectral range is 382-1026nm, spectral resolution is 2.8nm, sampling interval is 0.65nm, a total of 728 bands. The exposure time of the imaging system is 15ms, and the moving speed of the platform is 0.5mm/s.</p></p><ul><li>Finally, we obtained the original hyperspectral data of <strong>440 blades</strong>, and converted the hyperspectral data into <strong>RGB image data</strong> through python programming. The following figure shows a sample of our RGB data:</li></ul><p align = "center">    <img  src="/img/National/RGB-data.png" height="300" /></p><center>RGB image data</center><!-- * The leaves of the destructively sampled plants were dried and polished into powder, which was stored in a self-sealing bag for the determination of nitrogen content in the plants. The **nitrogen determination experiment** was carried out in the laboratory of College of Plant Protection, West Campus of China Agricultural University by distillation method. After five steps of deboiling, constant volume, alkalization, distillation and titration, the nitrogen content was obtained by calculating the acid standard liquid consumed during titration.  --><!-- The calculation formula is as follows: --><!-- ![实验测定氮含量](/img/National/nitrogen_content.png  "The nitrogen content was determined by experiment") --><!-- <p align = "center">    <img  src="/img/National/nitrogen_content.png" height="350" /></p><center>The nitrogen content was determined by experiment</center> --><p style="width:700px;" >    <img src="/img/National/nitrogen_content.png" style="border: none; box-shadow: none;" align="right" height = "200" hspace="10" vspace="10" >    <p>The leaves of the destructively sampled plants were dried and polished into powder, which was stored in a self-sealing bag for the determination of nitrogen content in the plants. The nitrogen determination experiment was carried out in the laboratory of College of Plant Protection, West Campus of China Agricultural University by distillation method. After five steps of deboiling, constant volume, alkalization, distillation and titration, the nitrogen content was obtained by calculating the acid standard liquid consumed during titration.  </p></p><h2 id="2-2-RGB-image-processing-and-feature-extraction"><a href="#2-2-RGB-image-processing-and-feature-extraction" class="headerlink" title="2.2 RGB image processing and feature extraction"></a>2.2 RGB image processing and feature extraction</h2><h4 id="2-2-1-Image-preprocessing"><a href="#2-2-1-Image-preprocessing" class="headerlink" title="2.2.1 Image preprocessing"></a>2.2.1 Image preprocessing</h4><!-- ![U-Net网络结构](/img/National/U-Net.png  "U-Net structure") --><p>Since the brightness of the RGB image after hyperspectral conversion is very low, and there are some whiteboard edges, in order to avoid noise pollution of the scanned image as much as possible, we preprocess all the RGB image data, mainly including: brightness enhancement, contrast enhancement, background segmentation and image cropping. In the process of image processing, the image segmentation algorithm based on U-Net was used to segment the main body of rapeseed leaves, and Abode Photoshop CC was used to separate each leaf separately, and the image background was uniformly adjusted to black to eliminate the influence of noise in the image background. The size of the adjusted image is 500 × 500px.</p><!-- ![处理前后对比](/img/National/Comparison.png  "Comparison of image samples before and after processing") --><p align = "center">    <img  src="/img/National/Comparison.png" height="320" /></p><center>Comparison of image samples before and after processing</center><h4 id="2-2-2-Qualitative-estimation-of-nitrogen-content-based-on-RGB-images"><a href="#2-2-2-Qualitative-estimation-of-nitrogen-content-based-on-RGB-images" class="headerlink" title="2.2.2 Qualitative estimation of nitrogen content based on RGB images"></a>2.2.2 Qualitative estimation of nitrogen content based on RGB images</h4><ul><li>The RGB images obtained after two crop planting experiments in this project were preprocessed, and more than 400 images were finally obtained. 10% images were used as the test set and the rest as the training set, which constituted the qualitative estimation data set of nitrogen content.</li><li>In the establishment of image classification model, this project considers VGGNet, ResNet, GoogleNet, AlexNet and other models, from which the optimal model is selected and improved. As can be seen from the table, VGG16 has a high accuracy rate, but there are still shortcomings.</li></ul><style>.center {  width: auto;  display: table;  margin-left: auto;  margin-right: auto;}</style><div class="center"><table><thead><tr><th align="center">Model</th><th align="center">Average accuracy</th></tr></thead><tbody><tr><td align="center">VGG16</td><td align="center">52%</td></tr><tr><td align="center">GoogleNet</td><td align="center">42%</td></tr><tr><td align="center">AlexNet</td><td align="center">34%</td></tr><tr><td align="center">ResNet34</td><td align="center">32%</td></tr></tbody></table></div><h4 id="2-2-3-Image-feature-extraction"><a href="#2-2-3-Image-feature-extraction" class="headerlink" title="2.2.3 Image feature extraction"></a>2.2.3 Image feature extraction</h4><ul><li>The scanned image of rapeseed leaves obtained by the experiment contains rich color and texture information. The above features extracted by image processing technology will become the main basis for the study of digital nitrogen nutrition estimation and modeling.</li><li>In this project, standardized RGB values are used to express color features to eliminate the influence of brightness information. At the same time, the RGB color model is converted to the HSI color model by geometric derivation to obtain the hue, saturation and brightness of the image.</li><li>In this project, a statistical texture feature, gray co-occurrence matrix (GLCM), was selected. GLCM is a method based on estimating the second-order combined conditional probability density of an image. Six texture features, including angular second moment (ASM), contrast (ContrastCon), Energy (Ene), Homogeneity (Hom), DissimilarityDis (DissimilarityDis) and Correlation (Cor), were extracted.</li></ul><h2 id="2-3-Hyperspectral-data-processing-and-feature-extraction"><a href="#2-3-Hyperspectral-data-processing-and-feature-extraction" class="headerlink" title="2.3 Hyperspectral data processing and feature extraction"></a>2.3 Hyperspectral data processing and feature extraction</h2><p>Hyperspectral images are rich in information. In this project, we first adopted principal component analysis (PCA) and feature wavelength extraction to preprocess hyperspectral data.</p><!-- ![高光谱数据处理](/img/National/Hyperspectral.png  "Hyperspectral data processing") --><p align = "center">    <img  src="/img/National/Hyperspectral.png" width="650" /></p><center>Hyperspectral data processing</center><h4 id="2-3-1-PCA-principal-component-analysis"><a href="#2-3-1-PCA-principal-component-analysis" class="headerlink" title="2.3.1 PCA(principal component analysis)"></a>2.3.1 PCA(principal component analysis)</h4><ul><li><p>Principal component analysis is an effective dimensionality reduction algorithm, which has been widely used in the field of spectral analysis. Principal component analysis is an attempt to explain as much information about the original variable as possible by using a new, unrelated set of variables.</p></li><li><p>The visual analysis of PC1~5 is carried out. As shown in the figure below, the contribution of PC1 and PC2 is relatively large.</p><!-- ![不同波段的贡献度](/img/National/different_bands.png  "The contribution of different bands") -->  <p align = "center">      <img  src="/img/National/different_bands.png" height="300" />  </p>  <center>The contribution of different bands</center></li><li><p>The connection between PC1 and PC2 is discussed separately. As shown in the figure below, the data points are relatively dense, which confirms the strong correlation between PC1 and PC2. In this project, PC1 and PC2 were selected to analyze the spectral characteristics.</p></li></ul>  <p align = "center">      <img  src="/img/National/PCA.png" height="300" />  </p><h4 id="2-3-2-Quantitative-estimation-of-nitrogen-content-based-on-hyperspectrum"><a href="#2-3-2-Quantitative-estimation-of-nitrogen-content-based-on-hyperspectrum" class="headerlink" title="2.3.2 Quantitative estimation of nitrogen content based on hyperspectrum"></a>2.3.2 Quantitative estimation of nitrogen content based on hyperspectrum</h4><p>In this project, support vector machine regression (SVR) algorithm was used to predict nitrogen in roselle. The advantage of SVR algorithm is that it introduces the concept of kernel function, through which the low-dimensional nonlinear problem is transformed into a high-dimensional space, so that it becomes a linear problem, which can solve the complex problem of multivariate variables.</p><!-- ![支持向量机回归](/img/National/support_vector.png  "support vector") --><p align = "center">    <img  src="/img/National/support_vector.png" height="300" /></p><center>support vector</center><p>As can be seen from the table, the accuracy of the model is significantly improved after PC1 and PC2 are selected as the main components, but there are still shortcomings.</p><div class="center"><table><thead><tr><th align="center">Data Set</th><th align="center">Accuracy</th><th align="center">Recall</th></tr></thead><tbody><tr><td align="center">original</td><td align="center">0.31</td><td align="center">0.33</td></tr><tr><td align="center">PC1, PC2</td><td align="center">0.54</td><td align="center">0.53</td></tr></tbody></table></div><h4 id="2-3-3-Feature-band-selection"><a href="#2-3-3-Feature-band-selection" class="headerlink" title="2.3.3 Feature band selection"></a>2.3.3 Feature band selection</h4><p>Hyperspectrum has the characteristics of large amount of data and strong redundancy between bands, which will have a certain impact on the later data processing and modeling, so it becomes very important to select some representative characteristic wavelengths. We take the absolute value of the average weight coefficient of each wavelength to obtain the actual contribution degree of each feature. The picture shows a visual image of characteristic band analysis of PC1:</p><!-- ![对PC1进行特征波段分析](/img/National/PC1_band.png  "PC1 characteristic band analysis")  --><p align = "center">    <img  src="/img/National/PC1_band.png" width="600" /></p><center>PC1 characteristic band analysis</center><p>As can be seen from the figure, the spectrum with the largest load, that is, the most contribution in the principal component PC1, is concentrated around 220. The wavelength is about 741~775, located in the red edge region. This result coincides with Sun Jun’s hyperspectral data analysis of lettuce, indicating that the characteristic band is closely related to crop nitrogen nutrition and has high analytical value. Therefore, we extracted the spectral data of all 260 leaf samples in this band as the spectral characteristics of the samples, and then stored the spectral data in matrix format to construct the hyperspectral data set of roeseed leaves at a specific wavelength (741-775).</p><h2 id="2-4-Fusion-hierarchical-model-based-on-convolutional-neural-network"><a href="#2-4-Fusion-hierarchical-model-based-on-convolutional-neural-network" class="headerlink" title="2.4 Fusion hierarchical model based on convolutional neural network"></a>2.4 Fusion hierarchical model based on convolutional neural network</h2><ul><li><p>Fusion model establishment: A feature fusion model based on convolutional neural network is designed to effectively fuse the features of RGB image and HSI data to improve image classification performance</p><!-- ![基于CNN的特征融合模型](/img/National/feature_fusion_model.png  "Feature fusion model based on CNN") --><p align = "center">    <img  src="/img/National/feature_fusion_model.png" width="900" /></p><center>Feature fusion model based on CNN</center></li><li><p>This project designed a feature fusion model based on convolutional neural network to effectively fuse the features of RGB image and HSI (Hyperspectral Imaging) data to improve image classification performance. The model uses CNN as the basic architecture, and carries on the corresponding design in the feature extraction part and the classifier part. </p></li><li><p>The following figure shows the change curve of accuracy and loss after running 1000 epochs. It can be seen from the figure that the accuracy of this model is finally stable at about 67%, and can reach nearly 80% at the highest time, which is better than the effect of single RGB image classification or hyperspectral modeling.</p></li></ul><!-- ![基于CNN的特征融合模型-acc](/img/National/Accuracy.png  "Accuracy")![基于CNN的特征融合模型-loss](/img/National/Loss.png  "Loss") --><p align = "center">    <img  src="/img/National/Accuracy.png" height="300" /></p><center>Accuracy</center><p align = "center">    <img  src="/img/National/Loss.png" height="300" /></p><center>Loss</center><h1 id="3-Innovation-Points"><a href="#3-Innovation-Points" class="headerlink" title="3. Innovation Points"></a>3. Innovation Points</h1><p><strong>1. The qualitative estimation of nitrogen content of roselle based on RGB image was realized</strong></p><ul><li>Establish multiple deep learning methods</li><li>To provide qualitative reference for nitrogen content in romaine rapeseed leaves</li></ul><p><strong>2. The quantitative estimation of nitrogen content in romaine was realized based on hyperspectrum</strong></p><ul><li>The spectral preprocessing and characteristic wavelength extraction were realized by statistical principle</li><li>The actual nitrogen content of plants was obtained by stoichiometry</li><li>A quantitative estimation model of nitrogen content in rosella was established with characteristic wavelength</li></ul><p><strong>3. A fusion estimation model of nitrogen stress in rosellas was established</strong></p><ul><li>A feature fusion model based on convolutional neural network is designed to fuse the features of RGB image and hyperspectral data effectively to improve the performance of image classification</li><li>Using CNN as the infrastructure and designing the feature extraction part and classifier part, the accuracy of nitrogen content estimation model is higher than that of single image or hyperspectral model</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
